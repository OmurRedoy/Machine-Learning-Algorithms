# -*- coding: utf-8 -*-
"""code_test_for_boot_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XxPmVdGOrrFPEtrOXYSay4KoKLgKU-6T
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn import metrics
from scipy import stats
import statsmodels.api as sm
import math
import re
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
import sys
import pandas.core.algorithms as algos
pd.pandas.set_option('display.max_columns',None)

from google.colab import drive
drive.mount('/content/drive')

news_data=pd.read_csv("/content/drive/MyDrive/Boot Camp-3/Data/Dataset.csv")

news_data

news_data.shape

news_data.isnull().sum()

news_data = news_data.fillna('')

news_data.isnull().sum()

news_data['content'] = news_data['Sentence']

print(news_data['content'])

# separating the data & label

## Get the Independent Features
X = news_data.drop(columns='Type', axis=1)
## Get the Dependent features
Y = news_data['Type']

Y.value_counts()

X.shape

Y.shape

print(X)
print(Y)

#separating the data and label
X = news_data['content'].values
Y = news_data['Type'].values

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit(X)
X = vectorizer.transform(X)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)

"""**Logistic**"""

from sklearn.metrics import accuracy_score
model = LogisticRegression()
model.fit(X_train, Y_train)
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

from sklearn import metrics
from sklearn.metrics import plot_confusion_matrix
import numpy as np
import mlxtend  
import itertools

model.fit(X_train, Y_train)
prediction = model.predict(X_test)
score = metrics.accuracy_score(Y_test, prediction)
print("accuracy:   %0.3f" % score)
cm1 = metrics.confusion_matrix(Y_test, prediction)
plot_confusion_matrix(model, X_test, Y_test)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(Y_test, prediction))

"""**Multinomial Naive**"""

#let's implement the model : Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train,Y_train)

from sklearn import metrics
import numpy as np
import itertools

classifier.fit(X_train, Y_train)
prediction1 = classifier.predict(X_test)
score = metrics.accuracy_score(Y_test, prediction1)
print("accuracy:   %0.3f" % score)
cm1 = metrics.confusion_matrix(Y_test, prediction1)
plot_confusion_matrix(classifier, X_test, Y_test)
plt.show()

X_new = X_test[0]

prediction = model.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

from sklearn.metrics import classification_report
print(classification_report(Y_test,prediction1))

"""**RF**"""

from sklearn.ensemble import RandomForestClassifier
rfmodel = RandomForestClassifier()
rfmodel.fit(X_train,Y_train)

from sklearn import metrics
import numpy as np
import itertools

rfmodel.fit(X_train, Y_train)
prediction2 = rfmodel.predict(X_test)
score = metrics.accuracy_score(Y_test, prediction2)
print("accuracy:   %0.3f" % score)
cm1 = metrics.confusion_matrix(Y_test, prediction2)
plot_confusion_matrix(rfmodel, X_test, Y_test)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(Y_test,prediction2))

"""SVM"""

from sklearn.svm import SVC
smodel = SVC()
smodel.fit(X_train,Y_train)

from sklearn import metrics
import numpy as np
import itertools

smodel.fit(X_train, Y_train)
prediction4 = smodel.predict(X_test)
score = metrics.accuracy_score(Y_test, prediction4)
print("accuracy:   %0.3f" % score)
cm1 = metrics.confusion_matrix(Y_test, prediction4)
plot_confusion_matrix(smodel, X_test, Y_test)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(Y_test,prediction4))

"""**KNN**"""

from sklearn.neighbors import KNeighborsClassifier
kmodel = KNeighborsClassifier()
kmodel.fit(X_train,Y_train)

from sklearn import metrics
import numpy as np
import itertools

kmodel.fit(X_train, Y_train)
prediction5 = kmodel.predict(X_test)
score = metrics.accuracy_score(Y_test, prediction5)
print("accuracy:   %0.3f" % score)
cm1 = metrics.confusion_matrix(Y_test, prediction5)
plot_confusion_matrix(kmodel, X_test, Y_test)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(Y_test,prediction5))

"""GradientBoosting"""

from sklearn.ensemble import GradientBoostingClassifier
gmodel = GradientBoostingClassifier()
gmodel.fit(X_train,Y_train)

from sklearn import metrics
import numpy as np
import itertools

gmodel.fit(X_train, Y_train)
prediction6 = gmodel.predict(X_test)
score = metrics.accuracy_score(Y_test, prediction6)
print("accuracy:   %0.3f" % score)
cm1 = metrics.confusion_matrix(Y_test, prediction6)
plot_confusion_matrix(gmodel, X_test, Y_test)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(Y_test,prediction6))